\documentclass{article}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{url}
\usepackage{amsmath}

\title{Project Report for CSE549 Fall 2012}
\author{Anirban Mitra \\
SBU ID \# 108672767 \\
{\tt anmitra@cs.stonybrook.edu} \\
\and
Ming Ma \\
SBU ID \# 108521588 \\
{\tt minma@cs.stonybrook.edu} \\
\and
Li Xindi \\
SBU ID \# 108729348}

\begin{document}
    \maketitle
    
    \section{Intorduction}
    
        We explored different algorithmic approaches for the problem of overlap detection between pairs of reads coming out of shotgun DNA sequencing. The shotgun sequencing is the most widely used technique for DNA sequencing. Overlap detection between reads is the first stage in DNA sequence assembly. Other stages after that are construction of multiple sequence alignments of reads, and generation of consensus sequences \\
        
        \begin{center}
            \includegraphics[scale=0.13]{stages.jpg}
        \end{center}
        
        But, in this project we only explored the problem of overlaps. Specifically, the new sequencing instruments from from Oxford Nanopore and Pacific Biosciences can sequence much longer reads, say upto 2000-5000 base pairs (bp), as compared to traditional sequencing which used to give reads of 100-500 bp. But, the gain is at the cost of high error rate of upto 15\%. All the experiments done are with simulated reads dataset for reads with such high error rates. Here are some highlights of the ideas explored 
        
        \begin{itemize}
            \item instead of matching overlaps per character basis, compare the reads block wise by partitioning the reads into blocks of characters and identifying blocks by unique id's
            \item memoization and on demand memoization for edit distance table of two blocks
            \item KMP kind of approach for the purpose of overlap 
        \end{itemize}
        
    \section{Dataset}
    
        The dataset used for the experiments is the simualted reads generated by Readsim (\url{http://sourceforge.net/projects/readsim/}). The simulated reads are from the first 70000 base pairs of bacteria Ecoli DNA sequence distributed as sample DNA along with Readsim source. The reads were generated by Readsim for PacBio kind of sequencing with similar high error rates along with 30x coverage. The total reads generated is more than a 1000. The reference overlaps against all overlaps generated by our approaches are compared with is generated by using Minimus (\url{http://www.biomedcentral.com/1471-2105/8/64}). For the purpose of comparing the differences between the reference overlaps and our calculated overlaps, we defined the following value
        
        \begin{align*}
            \text{Relative error for one pair} &= \frac{abs(\text{reference overlap} - \text{out overlap})} {\text{reference overlap}} \\
            Stddev &= \frac{\text{sum of relative errors of all pairs}}{\text{total number of pairs}}
        \end{align*}
        
    \section{Algorithms}
    
        If say there are $n$ reads and the reads are at maximum of length $l$ then the the naive dynamic algorithm like Smith-Waterman takes $O(l^2)$ time for each pair overlap detection. Hence the total time for all pairs of overlaps is $O(n^2)$. This is clearly very expensive large number of long reads coming out of PacBio which are of length more that 5000.
        
        \subsection{Naive Block Wise Match}
        
            We start with just trying approaches for optimising the overlap detection for a given pair of reads. The basic idea is to rather be able to do character wise matching, compare two blocks in one operation. For this purpose If we simply partition the sequence into blocks of size $k$ and give them unique id's. So, now we can compare two blocks by just comparing the id's. Say if $k = 5$ then there will be total of $4^5$ such unique blocks are possible and we can say assign them id'd from $1$ to $4^5$.
            
            Lets say the read are $x[1..5000], y[1..2000]$ where $x[i], y[i]$ is a base, then say my block is of size 20, then $x[1..20]$ is the first block, $x[21..40]$ second block, so on and similarly for $y$. So I first try to align $y[1..20] with x[1..20]$ if not then with $y[1..20] and x[21..40]$ then $y[1..20]$ and $x[41..60]$.
            
        \subsection{Naive DP}
        
            To put things in perspective, letâ€™s compare with runtimes for very simple Smith-Waterman DP on my machine
            
\begin{verbatim}
Reads count             Time on my VM

10                      0m1.477s
20                      0m15.762s
50                      1m5.913s
100                     3m21.271s
\end{verbatim}

        \subsection{Optimised Block Wise Match}
            
            Now, this is obvious that we will miss many of the overlaps with the above simple approach. This match will only work for the cases when the overlaps start from a block boundary but will fail in case the overlap starts from not block boundary but from in between. In other words, $y[1..20]$ may start to overlap with a block of $x$ from the middle say, $y[1..10]$ matches with $x[31..40]$ and so on.
            
            To get around this we give an initial credit so that the algorithm can survive high block mismatches initially. So, for example in the case where $y[1..10]$ overlaps with $x[31..40]$, then the match between $x[1..20]$ and $y[21..40]$ will pass due to the initial credit given. In the future matches we keep adding/reducing the credit accordingly to the matches/mismatches of the of further blocks. If the credit goes below a certain threshold then assume that particular overlap as bad and break off. We assumed an inital credit equal to the block size for our experiments.

            There is another complication to this. The overlaps between the $x$ and $y$ reads may continue with a particular shift from the block boundary, so how to cope up with that? So, what we do is that I actually take the best overlap of $x[1..20]$ with $y[1..18], y[1..19], y[1..20], y[1..21] and y[1..22]$ since we expect an error rate of 15\%. So for a given block of size 20 the total error of upto 2 insertions, deletion or substitutions are expected. Hence, those becomes the candidates for a block match. So in the case where $x[31..40]$ overlaps with $y[1..10]$, $x[41..60]$ with $y[11..30]$ and so on, we will get $x[31..40]$ overlap with $y[1..18]$ using our algorithm, $x[41..60]$ with $y[19..36]$, $x[61..80]$ with $y[37..54]$ and so on. So, the matches will keep shifting till it reaches the correct alignment.

            With this kind of scheme we get overlaps in multiples of the block size only so if the overlap is of say 256 b.p. and the block size is 5 then we will get an overlap of 250. I actually checked the results out of my program are within this range when compared with the output of minimus for the same input. Only in rare instances they differ a lot.
            
\begin{verbatim}
Block Size      Stdev       1% Relative Error       10% Relative Error

100             0.140014    216/379                 336/379
200             0.178973    572/989                 861/989
500             0.249012    1787/3291               2766/3291
1000            0.263768    3552/6601               5540/6601

\end{verbatim}

        \subsection{Naive Block Wise DP}
        
            Initially, I was calculating the alignment between blocks each time using DP by Smith-Waterman. I varied the block size to see the effect on runtime and here it is

\begin{verbatim}
Block Size          Time taken

24                  0m5.787s
18                  0m4.533s
12                  0m3.330s
6                   0m2.237s
\end{verbatim}

            So, now with block size of $k=5$, I hash each possible DNA sequence of length 5 to an integer id and pre-calculate the overlap cost of each pair of blocks. Here is the runtimes for different number of reads

\begin{verbatim}
Reads count             Time on my VM

100                     0m6.946s
200                     0m12.571s
500                     0m59.926s
1000                    3m47.140s
\end{verbatim}

        \subsection{Memoization}
        
            Since it is clear that the block wise matching is faster for smaller block sizes then, we might as well precompute the edit distances between all pairs of possible blocks and save them in a table. So, now we can directly use them during our algorithm rather than computing the edit distance between the candidate blocks every time. So, I precomputed the edit distances for all the possible block pairs for $k=5$. There will be around $n = 4^5$ possible blocks, so the possible pairs are $n^2 = 10^6$. It is possible to save such a table in memory. Obviously, this memoization trick cannot be used for bigger block sizes. The time taken for initiating the table is around $0m4.871s$ which is quite ok when compared to the small data set that we are using but ultimately with very large data set we will stand to gain from it as it is a one time constant pre-processing cost.
            
\begin{verbatim}
Reads count             Time on my VM

100                     0m6.004s
200                     0m11.260s
500                     0m40.257s
1000                    2m27.025s
\end{verbatim}

            It is clear that with increasing number of reads the memoised algorithm is performing much better. Probably, because for such a small block size $k=5$ we are getting this much benefit, with larger block size the savings will be much greater but that will also require more memory (memory on my machine will not be sufficient for this because the memory requirement increases in quadratic with $k$).
            
        \subsection{On Demand DP with Memoization}
        
            Lets try on demand memoization rather than full table calculation at the beginning. What I mean is that we calculate the edit distance between two blocks when asked and save it so that if required again we don't have to recalculate it. The runtime for that is
            
\begin{verbatim}
Reads count             Time on my VM

100                     0m2.466s
200                     0m6.107s
500                     0m30.066s
1000                    1m58.545s
\end{verbatim}
            
          This is wonderful. All the optimisations are paying off :).
            
        \subsection{Fast Overlap Rejection with Set Matching}
            
            One more optimisation that is possible is that before going to actually test a particular sequence overlap between two reads, one does a very fast test on the set of the block id's for the overlap regions. If the set of block id's from the two regions is very different then for sure the sequences cannot match. This test can be used to reject very poor overlaps very fast and save on the runtime.
            
        \subsection{Why not KMP?}
        
            Can we modify KMP for finding overlaps? It turns out tat we can with exact overlaps. We just need to modify the terminating condition of KMP so that to not terminate in case we do not get the complete pattern in the text but goes on to to try matching prefixes of pattern. In fact the classical KMP is an extension of suffix prefix matching of two strings.

            But what about approximate matches? Naively, it looks like we can modify it to handle substitutions but not insertion and deletions. But for now lets just consider substitutions only, say at max $k$ substitution. So, during preprocessing of the pattern in KMP we build a table which calculates, for every index, consider the string $P[1..i]$, find the largest proper prefix, $P[1..j]$, $j<i$, which matches the suffix $P[i-j+1..i]$. In our case the match may contain upto $k$ mismatches. Consider building the table for the pattern ABABA for $k=3$

\begin{verbatim}
P[1..1) = "A"
Prefix = P[1..1) = ""
Match Size = 0

P[1..2) = "AB"
Prefix = P[1..1] = "A"
Match Size = 1

P[1..3) = "ABA"
Prefix = P[1..2] = "AB"
Match Size = 2

P[1..4) = "ABAB"
\end{verbatim}

            For this it will first try to extend the last match. So it will try to match $"ABA"$ as suffix of $"ABAB"$ when the mismatches will become 3 and will be rejected. Next it will try with the prefix match of $"AB"$ which is $"A"$ and will accept this as it is of only 1 mismatch. But we skipped the optimal match, because we do not have $"A"$ match of $P[1..3)$ which is rejected in favour of larger match $"AB"$ even if with more mismatches. 

            To overcome this even if we may keep a list of matches for every index. We will have to keep $k$ matches in order in which case the runtime will become $O(nk)$ which is the same as DP. So, this is not helping.

    \section{Code}
    
        I have attached the code written in the appendix. Alternatively, all the code is also on Github at \url{https://github.com/nomind/CompBioFall2012}.
        
    \section{Conclusion}
        
        It is clear that the approach of block wise overlap matching is definitely much much better than naive DP Smith-Waterman. But this is not comparable to actual systems like Minimus. The Minimus takes about 26s to detect the overlaps for 1000 reads which far better. But it is not clear that how much the removal of poor regions of each read done before the overlap detection part by systems like Minimums has to contribute to this. Hence, it is not strictly comparable with the results in this report. But it would be really interesting to see everything else being same, how does this compare to the implementation of Minimus, which I believe is an optimised implementation of Smith-Waterman only.
        
\end{document}


